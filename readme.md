# Variational inference on a normal distribution

This project implements a basic example of variational inference. We infer the posterior mean and variance of sampled real valued numbers. For this model, we also know the exact form for the posterior. Therefore, it helps us to understand how variational inference works. Later, we can use this knowledge for more complicated models.

# What is variational inference?
In variational inference (VI), we approximate a complicated distribution by a simpler approximating distribution. Throughout literature, the true distribution is referred to as <img alt="$p(x)$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/c9ea84eb1460d2895e0cf5125bd7f7b5.svg?invert_in_darkmode" align=middle width="30.33723pt" height="24.56552999999997pt"/> and the approximating distribution as <img alt="$q$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/d5c18a8ca1894fd3a7d25f242cbe8890.svg?invert_in_darkmode" align=middle width="7.898533500000002pt" height="14.102549999999994pt"/>, we will also use this notation. In our case, the distributions <img alt="$p$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width="8.239720500000002pt" height="14.102549999999994pt"/> and <img alt="$q$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/d5c18a8ca1894fd3a7d25f242cbe8890.svg?invert_in_darkmode" align=middle width="7.898533500000002pt" height="14.102549999999994pt"/> represent the posterior over the parameters over our model. In this case, people refer to VI as variational Bayes. 

The distribution <img alt="$q$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/d5c18a8ca1894fd3a7d25f242cbe8890.svg?invert_in_darkmode" align=middle width="7.898533500000002pt" height="14.102549999999994pt"/> will approximate <img alt="$p$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width="8.239720500000002pt" height="14.102549999999994pt"/> according to the KL-divergence between the distributions. 

<img alt="$KL(q||p) = \int q(x) log \frac{q(x)}{p(x)} dx$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/b79c7ae8eb0f0202bbe763810e7f5277.svg?invert_in_darkmode" align=middle width="197.65729499999998pt" height="33.14091000000001pt"/>

A high KL divergence means that the distributions are quite different. A low KL divergence means that the distributions look more like eachother. So as we minimize the KL divergence, the distribution <img alt="$q$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/d5c18a8ca1894fd3a7d25f242cbe8890.svg?invert_in_darkmode" align=middle width="7.898533500000002pt" height="14.102549999999994pt"/> will approximate <img alt="$p$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width="8.239720500000002pt" height="14.102549999999994pt"/>.

## Reverse KL divergence ?
A natural question follows on why we minimize <img alt="$KL(q||p)$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/96488d0599b0a17bc51f40c74de2539e.svg?invert_in_darkmode" align=middle width="64.206615pt" height="24.56552999999997pt"/> and not <img alt="$KL(p||q)$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/ec0e9578244feb36319f89e9cb157a53.svg?invert_in_darkmode" align=middle width="64.206615pt" height="24.56552999999997pt"/>? In calculating <img alt="$KL(p||q), we need samples from $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/10581e29418feb99570355cdbc47dd54.svg?invert_in_darkmode" align=middle width="224.30809499999998pt" height="24.56552999999997pt"/>p<img alt="$. However, for the complicated distributions that we want to approximate, it is hard to sample. Either because we have no closed from expression to sample or because the normalization constant is intractable to compute. Therefore, we minimize $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/53f6860e7f80ba4c481b8b111b66df8c.svg?invert_in_darkmode" align=middle width="1649.632545pt" height="22.745910000000016pt"/>KL(q||p)<img alt="$. We pick a simple family of distributions, $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/c862a92a49a4c3840c7f241c956c1625.svg?invert_in_darkmode" align=middle width="286.58569500000004pt" height="22.745910000000016pt"/>q<img alt="$, for which we know how to generate samples. [Section 21.2.2 in Murphy](https://mitpress.mit.edu/books/machine-learning-0) has some nice comments on this.&#10;&#10;# Our model&#10;We want to infer the posterior over the parameters of a 1D Gaussian. For the precision of the Gaussian, we use a Gamma prior. The model writes down as&#10;&#10;$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/d56f5bb99fa5fb77287bb4c4ce05ca5a.svg?invert_in_darkmode" align=middle width="852.3801000000001pt" height="124.72482000000002pt"/>p(\mu, \lambda) = \mathcal{N}(\mu| \mu_0, (\kappa \lambda)^{-1})\mathcal{G}amma(\lambda| a_0, b_0)<img alt="$&#10;&#10;Our approximate posterior for this model will use the mean field approximation. In mean field approximations, we assume that the posterior factorizes over each of the variables. So $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/de0815acccf928a03b264746604b7657.svg?invert_in_darkmode" align=middle width="698.02095pt" height="85.27266pt"/>q(x)<img alt="$ is a product of independent $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/2d9bbc84867c6077cc32bac9bb1f222a.svg?invert_in_darkmode" align=middle width="183.835245pt" height="22.745910000000016pt"/>q_i(x_i)<img alt="$. Each variable gets its own factor. &#10;&#10;For our model, this implies that we use a separate approximation for $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/18140982569d6711290ccf9bff385c34.svg?invert_in_darkmode" align=middle width="486.433695pt" height="45.82082999999998pt"/>\mu<img alt="$ and for $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/4d441f8ba0cae704d9fa86cdfa68908c.svg?invert_in_darkmode" align=middle width="49.84023pt" height="22.745910000000016pt"/>\lambda<img alt="$. Our $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/8f2bac52f4e7dc464eccf1ca7bd5acf1.svg?invert_in_darkmode" align=middle width="34.716495pt" height="22.381919999999983pt"/>q<img alt="$ writes as&#10;&#10;$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/340ed53f5e9fcc542f350838042b860f.svg?invert_in_darkmode" align=middle width="63.203745pt" height="21.602129999999985pt"/>q(\mu, \lambda) = q(\lambda)q(\mu)<img alt="$&#10;&#10;# Minimize the KL divergence&#10;Now that we specified our model and approximation, we want to minimize the KL divergence. We assumed that the approximations factorizes over the individual parameters, so our objective is: &#10;&#10;$" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/12e9d7015a2db04cccab405727c4893b.svg?invert_in_darkmode" align=middle width="698.02755pt" height="124.72482pt"/>\min_{q(\mu, \lambda)} KL(q||p) \rightarrow \min_{q(\mu), q(\lambda)} KL(q||p)<img alt="$&#10;&#10;We will minimize this objective with coordinate descent. That means we minimize the objective for one $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/320cca428f823836b7e058a8e02c758d.svg?invert_in_darkmode" align=middle width="697.9549499999999pt" height="78.90399000000001pt"/>q_i<img alt="$ at a time. When we loop this long enough, we will find a (local) minimum of the objective. In [Murphy, section 25.5.1], this coordinate descent algorithm is derived for our model. Our code implements these equations.&#10;&#10;# The lower bound&#10;In our coordinate descent algorithm, we minimize the objective $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/c3f2cbe2b6381e47dd36040b65be159c.svg?invert_in_darkmode" align=middle width="1451.5236449999998pt" height="45.82083000000002pt"/>L(q) = KL(q(\mu)q(\lambda)||p(\mu, \lambda))<img alt="$. We can use this quantity to sanity check our implementation. Coordinate descent guarantees us that the objective can only decrease or stay equal at each step. Therefore, we track $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/ea3ddd227e8535d75f67a6499c997ff6.svg?invert_in_darkmode" align=middle width="1201.249995pt" height="22.745910000000016pt"/>L<img alt="$ throughout our optimization. If we observe an increase, then we know that our implementation is incorrect.&#10;&#10;The value of $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/4a4e2a75109ef952cf7df17ad48d0932.svg?invert_in_darkmode" align=middle width="733.620195pt" height="39.45182999999998pt"/>L(q)<img alt="$ has a second function. It turns out that $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/2bdc8bf47c7b900788ccae6d6c45e7ec.svg?invert_in_darkmode" align=middle width="258.51259500000003pt" height="22.745910000000016pt"/>L<img alt="$ is a lower bound on the model evidence. In other words, we know that $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/b1680992a329e01b6b1b9eb0ae606cf6.svg?invert_in_darkmode" align=middle width="454.123395pt" height="22.745910000000016pt"/>L(q) = KL(q(\mu)q(\lambda)||p(\mu, \lambda)) \leq log \ p(\mathcal{D}ata)<img alt="$. As such, we can use the value of $" src="https://rawgit.com/RobRomijnders/vi_normal/master/svgs/9282a70c7f36983fd4bb8c87f7e482b3.svg?invert_in_darkmode" align=middle width="215.505345pt" height="22.745910000000016pt"/>L$ for model selection.

# Results




# Further reading

  * [Chapter 21 in Murphy on Variational Inference](https://mitpress.mit.edu/books/machine-learning-0)

      * This project draws inspiration from section 21.5.1